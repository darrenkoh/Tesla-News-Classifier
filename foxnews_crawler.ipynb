{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import bs4\n",
    "import typing\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global variables\n",
    "catalog_filename = 'foxnews_catalog.json'\n",
    "news_filename = 'foxnes_news.json'\n",
    "url_foxnews_base = 'https://www.foxnews.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def FilterCatalogs(current: set(), new: []) -> []:\n",
    "    output = []\n",
    "    for n in new:\n",
    "        if url_foxnews_base + n['url'] not in current:\n",
    "            output.append(n)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Foxnews side for Tesla specific news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing Catalogs\n",
    "catalog = None\n",
    "catalogUrls = set()\n",
    "with open(catalog_filename, 'r') as infile:\n",
    "    catalog = json.load(infile)\n",
    "    catalogUrls = set([url_foxnews_base+c['url'] for c in catalog])\n",
    "\n",
    "url_foxnews = 'https://www.foxnews.com/api/article-search'\n",
    "query = {\n",
    "    \"isTag\" : 'true',\n",
    "    \"searchSelected\" : \"fox-news/auto/make/tesla\",\n",
    "    \"size\": 30,\n",
    "    \"offset\" : 0\n",
    "}\n",
    "\n",
    "prevCatalogCount = len(catalog)\n",
    "for x in tqdm(range(0,1000), ncols=45):\n",
    "    query['offset'] = x * 30\n",
    "    r = requests.get(url_foxnews, params=query)\n",
    "    if r.status_code == 200:\n",
    "        if r.text != '[]':\n",
    "            response = r.json()\n",
    "            newCatalogs = FilterCatalogs(catalogUrls, response)\n",
    "            if len(newCatalogs) > 0:\n",
    "                catalog.extend(newCatalogs)\n",
    "            else:\n",
    "                print(\"There is no more news to add, coz latest downloaded news are already in the catalogs\")\n",
    "                print(f\"Last iteration: {x} (query: {query})\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Got blank response at iteration {x*30}\")\n",
    "            break\n",
    "    time.sleep(10)\n",
    "    \n",
    "print(f\"Total new articles: {len(catalog) - prevCatalogCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the catalog\n",
    "with open(catalog_filename, 'w') as outfile:\n",
    "    json.dump(catalog, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the text content from the foxnews catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def SanitizerAdWords(content: str) -> str:\n",
    "    content = content.lower()\n",
    "    for a in ad_words:\n",
    "        content = content.replace(a,'')\n",
    "    return content\n",
    "\n",
    "def SanitizerHtml(content: str) -> str:\n",
    "    temp = unicodedata.normalize(\"NFKD\",content.rstrip('.').strip()) + '.'\n",
    "    output = re.sub(u\"(\\u2018|\\u2019)\", \"'\", temp)\n",
    "    output = re.sub(u\"(\\u2013|\\u2014)\", \"-\", output)\n",
    "    output = re.sub(u\"(\\u201c|\\u201d)\", '\"', output)\n",
    "    output = re.sub(u\"(\\u200b)\", '', output)\n",
    "    return output\n",
    "    \n",
    "# Remove unrelated content - specific to fox news\n",
    "def SanitizerNonArticleTags(tag: bs4.element.Tag):\n",
    "    featured = tag.find(attrs={\"class\":\"featured featured-video video-ct\"})\n",
    "    if featured:\n",
    "        featured.decompose()\n",
    "\n",
    "def GetContent(url: str) -> typing.Dict[str,str]:\n",
    "    content = None\n",
    "    r = requests.get(url)\n",
    "    output = {\"body\":'', \"authorName\":'', \"authorUrl\":''}\n",
    "    if r.status_code == 200:        \n",
    "        html = BeautifulSoup(r.text, 'html.parser')\n",
    "        article_title = SanitizerHtml(html.find(attrs={\"class\": \"headline\"}).text)\n",
    "        article_body = html.find(attrs={\"class\": \"article-body\"})\n",
    "        article_author = html.find(attrs={\"class\": \"author-byline\"}).find(\"a\")\n",
    "        if article_author:            \n",
    "            output['author_url'] = article_author['href']\n",
    "            output['author_name'] = article_author.text\n",
    "        SanitizerNonArticleTags(article_body)\n",
    "        content = article_title + '. ' + ' '.join([SanitizerHtml(x.text) for x in article_body.find_all('p')])\n",
    "        content = SanitizerAdWords(content)        \n",
    "        content = unicodedata.normalize(\"NFKD\",content)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog for retrieving the news content \n",
    "catalog = None\n",
    "with open(catalog_filename, 'r') as infile:\n",
    "    catalog = json.load(infile)\n",
    "    \n",
    "# Load the current news\n",
    "news = []\n",
    "if path.exists(news_filename):\n",
    "    with open(news_filename, 'r') as infile:\n",
    "        news = json.load(infile)\n",
    "    \n",
    "# Ads Words that we want to remove from the news content\n",
    "ad_words = None\n",
    "with open('ad_words.txt', 'r') as infile:\n",
    "    ad_words = [line.rstrip('\\n') for line in infile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.foxnews.com/auto/teslas-semitruck-%e2%94%80-what-to-expect'\n",
    "# r = requests.get(url)\n",
    "# html = BeautifulSoup(r.text, 'html.parser')\n",
    "# article_author = html.find(attrs={\"class\": \"author-byline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://www.foxnews.com/auto/teslas-semitruck-%e2%94%80-what-to-expect'\n",
    "# aaa = print(GetContent(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_urlcache = set([n['url'] for n in news])\n",
    "for inx in tqdm(range(len(catalog))):\n",
    "    url = url_foxnews_base + catalog[inx]['url']\n",
    "    if url not in news_urlcache:\n",
    "        #print(f'new url not in cache {url}')\n",
    "        content = GetContent(url)\n",
    "        news.append({'url': url, 'body':content, 'sentiment': ''})\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the news \n",
    "with open(news_filename, 'w') as outfile:\n",
    "    json.dump(news, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
