{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json, csv\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import bs4\n",
    "import typing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global variables\n",
    "catalog_filename = 'foxnews_catalog.json'\n",
    "news_filename = 'foxnews_news.json'\n",
    "url_foxnews_base = 'https://www.foxnews.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def FilterCatalogs(current: set(), new: []) -> []:\n",
    "    output = []\n",
    "    for n in new:\n",
    "        if url_foxnews_base + n['url'] not in current:\n",
    "            output.append(n)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl the Foxnews side for Tesla specific news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing Catalogs\n",
    "catalog = None\n",
    "catalogUrls = set()\n",
    "with open(catalog_filename, 'r') as infile:\n",
    "    catalog = json.load(infile)\n",
    "    catalogUrls = set([url_foxnews_base+c['url'] for c in catalog])\n",
    "\n",
    "url_foxnews = 'https://www.foxnews.com/api/article-search'\n",
    "query = {\n",
    "    \"isTag\" : 'true',\n",
    "    \"searchSelected\" : \"fox-news/auto/make/tesla\",\n",
    "    \"size\": 30,\n",
    "    \"offset\" : 0\n",
    "}\n",
    "\n",
    "prevCatalogCount = len(catalog)\n",
    "for x in tqdm(range(0,1000), ncols=45):\n",
    "    query['offset'] = x * 30\n",
    "    r = requests.get(url_foxnews, params=query)\n",
    "    if r.status_code == 200:\n",
    "        if r.text != '[]':\n",
    "            response = r.json()\n",
    "            newCatalogs = FilterCatalogs(catalogUrls, response)\n",
    "            if len(newCatalogs) > 0:\n",
    "                catalog.extend(newCatalogs)\n",
    "            else:\n",
    "                print(\"There is no more news to add, coz latest downloaded news are already in the catalogs\")\n",
    "                print(f\"Last iteration: {x} (query: {query})\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Got blank response at iteration {x*30}\")\n",
    "            break\n",
    "    time.sleep(10)\n",
    "    \n",
    "print(f\"Total new articles: {len(catalog) - prevCatalogCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the catalog\n",
    "with open(catalog_filename, 'w') as outfile:\n",
    "    json.dump(catalog, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the text content from the foxnews catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def SanitizerAdWords(content: str) -> str:\n",
    "    content = content.lower()\n",
    "    for a in ad_words:\n",
    "        content = content.replace(a,'')\n",
    "    return content\n",
    "\n",
    "def SanitizerHtml(content: str) -> str:\n",
    "    temp = unicodedata.normalize(\"NFKD\",content.rstrip('.').strip()) + '.'\n",
    "    output = re.sub(u\"(\\u2018|\\u2019)\", \"'\", temp)\n",
    "    output = re.sub(u\"(\\u2013|\\u2014)\", \"-\", output)\n",
    "    output = re.sub(u\"(\\u201c|\\u201d)\", '\"', output)\n",
    "    output = re.sub(u\"(\\u200b)\", '', output)\n",
    "    return output\n",
    "    \n",
    "# Remove unrelated content - specific to fox news\n",
    "def SanitizerNonArticleTags(tag: bs4.element.Tag):\n",
    "    for classPath in [\n",
    "        \"featured featured-video video-ct\", \n",
    "        \"speakable\"\n",
    "        ]:\n",
    "        x = tag.find(attrs={\"class\":classPath})\n",
    "    if x:\n",
    "        x.decompose()\n",
    "\n",
    "def GetContent(url: str) -> typing.Dict[str,str]:\n",
    "    content = None\n",
    "    r = requests.get(url)\n",
    "    output = {\"body\":'', \"authorName\":'', \"authorUrl\":''}\n",
    "    if r.status_code == 200:        \n",
    "        html = BeautifulSoup(r.text, 'html.parser')\n",
    "        article_title = SanitizerHtml(html.find(attrs={\"class\": \"headline\"}).text)\n",
    "        article_body = html.find(attrs={\"class\": \"article-body\"})\n",
    "        article_author = html.find(attrs={\"class\": \"author-byline\"}).find(\"a\")    \n",
    "        if article_author:\n",
    "            output['authorName'] = article_author['href']\n",
    "            output['authorUrl'] = article_author.text\n",
    "        SanitizerNonArticleTags(article_body)\n",
    "        content = article_title + '. ' + ' '.join([SanitizerHtml(x.text) for x in article_body.find_all('p')])\n",
    "        content = SanitizerAdWords(content)        \n",
    "        content = unicodedata.normalize(\"NFKD\",content)\n",
    "        output['body'] = content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the catalog for retrieving the news content \n",
    "catalog = None\n",
    "with open(catalog_filename, 'r') as infile:\n",
    "    catalog = json.load(infile)\n",
    "    \n",
    "# Load the current news\n",
    "news = []\n",
    "if path.exists(news_filename):\n",
    "    with open(news_filename, 'r') as infile:\n",
    "        news = json.load(infile)\n",
    "        # if news has no body, remove it from cache so we can try to download the body again\n",
    "        for idx in range(len(news)-1, -1, -1):\n",
    "            if not news[idx]['body']:\n",
    "                news.pop(idx)\n",
    "    \n",
    "# Ads Words that we want to remove from the news content\n",
    "ad_words = None\n",
    "with open('ad_words.txt', 'r') as infile:\n",
    "    ad_words = [line.rstrip('\\n') for line in infile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://www.foxnews.com/auto/teslas-semitruck-%e2%94%80-what-to-expect'\n",
    "# r = requests.get(url)\n",
    "# html = BeautifulSoup(r.text, 'html.parser')\n",
    "# article_author = html.find(attrs={\"class\": \"author-byline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://www.foxnews.com/auto/teslas-new-roadster-will-deliver-hardcore-smackdown-to-gasoline-cars-musk'\n",
    "# aaa = print(GetContent(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_urlcache = set([n['url'] for n in news])\n",
    "for inx in tqdm(range(len(catalog))):\n",
    "    url = url_foxnews_base + catalog[inx]['url']\n",
    "    if url not in news_urlcache:\n",
    "        print(f'new url not in cache {url}')\n",
    "        content = GetContent(url)\n",
    "        news.append({'url': url, 'body':content['body'], 'authorUrl':content['authorUrl'], 'authorName':content['authorName'], 'sentiment': ''})\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the news to json\n",
    "with open(news_filename, 'w') as outfile:\n",
    "    json.dump(news, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "data = pd.read_json('foxnews_news.json')\n",
    "print(data.sentiment.value_counts())\n",
    "print(data[data.sentiment == ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the news to csv \n",
    "df = pd.DataFrame(news)\n",
    "df.to_csv('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to training/test set\n",
    "df = pd.DataFrame(news)\n",
    "df = df.loc[df['sentiment'] != 'NA', ['body','sentiment']]\n",
    "label={'neutral':0,'positive':1,'negative':2}\n",
    "df['labels']=df['sentiment'].map(label)\n",
    "\n",
    "# Split the train/test set\n",
    "train, test = train_test_split(df[['body','labels']], test_size=0.2, random_state=18, shuffle=True)\n",
    "train.columns = ['text','labels']\n",
    "test.columns = ['text','labels']\n",
    "train.to_csv('data/train.csv', sep=',', index=False)\n",
    "test.to_csv('data/test.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  labels\n",
      "224  police report says tesla on autopilot sped up,...       2\n",
      "246  will your smartphone replace your car key?. th...       2\n",
      "251  tesla's new electric semi truck makes its debu...       1\n",
      "397  tesla's autopilot 8.0 protects kids and pets l...       1\n",
      "57   chuck devore: elon musk's tesla to texas? cali...       0\n",
      "                                                  text  labels\n",
      "109  is this what tesla's 'cybertruck' pickup will ...       0\n",
      "370  tesla pays off government loan nine years earl...       1\n",
      "51   elon musk lists 5 more homes for $97 million a...       2\n",
      "371  consumer reports gives tesla model s highest r...       1\n",
      "221  elon musk and malala are twitter bffs. (ap/spa...       0\n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         elon musk lists 5 more homes for $97 million a...\n",
       "sentiment                                             negative\n",
       "y                                                            2\n",
       "Name: 51, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
