{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-honolulu",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import PegasusTokenizer, BigBirdPegasusForConditionalGeneration, BigBirdPegasusConfig\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_json('foxnews_news_cleaned.json')\n",
    "# model = BigBirdPegasusForConditionalGeneration.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "# tokenizer = PegasusTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "# ARTICLE_TO_SUMMARIZE = df.body.loc[0] #\"My friends are cool but they eat too many carbs.\"\n",
    "# inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=4096, return_tensors='pt', truncation=True)\n",
    "# # Generate Summary\n",
    "# summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n",
    "# print(ARTICLE_TO_SUMMARIZE)\n",
    "# print([tokenizer.decode(g, skip_special_tokens=False, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "committed-vacuum",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-canada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 uninstall transformers -y\n",
    "#!pip3 install git+https://github.com/huggingface/transformers\n",
    "#!pip3 install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('foxnews_news_cleaned.json')\n",
    "print(df.sentiment.value_counts(normalize=True))\n",
    "summarizer = pipeline('summarization', model=\"google/bigbird-pegasus-large-arxiv\")\n",
    "abstract = summarizer(df.body.loc[0], truncation='longest_first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-girlfriend",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('foxnews_news_cleaned.json')\n",
    "model_name = 'google/pegasus-xsum'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Generate in small batches else running out of memory on GTX 1080Ti :(\n",
    "summary = []\n",
    "chunkSize = 5\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(range(0,df.shape[0], chunkSize), ncols=100):\n",
    "        textChunk = df.body.loc[x:x+chunkSize-1]\n",
    "        batch = tokenizer([f'\\\"{t}\\\"' for t in textChunk], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "        translated = model.generate(**batch)\n",
    "        summary.extend(tokenizer.batch_decode(translated, skip_special_tokens=True))\n",
    "df['summary'] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-teddy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json('foxnews_news_cleaned.json')\n",
    "# model_name = 'google/pegasus-xsum'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "# model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "# batch = tokenizer([df.body.loc[20]], truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "# translated = model.generate(**batch)\n",
    "# tokenizer.batch_decode(translated, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-section",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.summary.value_counts().nlargest(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-duncan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the cleaned data\n",
    "parsed = json.loads(df.to_json(orient='records', force_ascii=False, indent=4))\n",
    "with open('foxnews_news_cleaned_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-brunei",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('foxnews_news_cleaned_summary.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-appearance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = df[df.summary == 'All images are copyrighted.']\n",
    "print(a)\n",
    "a.loc[20].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to training/test set\n",
    "df = pd.read_json('foxnews_news_cleaned_summary.json')\n",
    "df = df.loc[df['sentiment'] != 'NA', ['summary', 'sentiment']]\n",
    "label = {'neutral':0,'positive':1,'negative':2}\n",
    "df['labels'] = df['sentiment'].map(label)\n",
    "\n",
    "# Split the train/test set\n",
    "train, test = train_test_split(df[['summary', 'labels']], test_size=0.2, random_state=12, shuffle=True)\n",
    "train.columns = ['text','labels']\n",
    "test.columns = ['text','labels']\n",
    "train.to_csv('data/train.csv', sep=',', index=False)\n",
    "test.to_csv('data/test.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-learning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
